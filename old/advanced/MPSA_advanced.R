# ============================================================================
# Advanced Multivariate Proximity-based Spatial Autocorrelation (MPSA) Framework
# 
# ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” ë¹„ì§€ë„ Random Forestì˜ proximity matrixë¥¼ í™œìš©í•œ 
# í˜ì‹ ì ì¸ ê³µê°„ ë¶„ì„ ë°©ë²•ë¡ ì„ êµ¬í˜„í•©ë‹ˆë‹¤.
# 
# ì£¼ìš” íŠ¹ì§•:
# 1. Multi-scale MPSA ë¶„ì„
# 2. Spatio-temporal MPSA í™•ì¥
# 3. Bayesian MPSA ì¶”ë¡ 
# 4. Network-based MPSA
# 5. ê³ ê¸‰ ì‹œê°í™” ë° í•´ì„ ë„êµ¬
# ============================================================================

# --- í™˜ê²½ ì„¤ì • ---
source("R/data_preparation/setup.R")
library(igraph)
library(ggraph)
library(viridis)
library(patchwork)
library(broom)
library(gstat)
library(spacetime)
library(spBayes)

# === 1. ì´ë¡ ì  í”„ë ˆì„ì›Œí¬ ===================================================

#' Global Multivariate Proximity-based Spatial Autocorrelation (GMPSA)
#' 
#' @description ì „ì—­ì  ê³µê°„ ì—°ê´€ì„± ì¸¡ì •ì„ ìœ„í•œ GMPSA í†µê³„ëŸ‰
#' @param P proximity matrix from random forest
#' @param W spatial weight matrix
#' @return GMPSA statistic and p-value
compute_GMPSA <- function(P, W, n_perm = 999) {
  # GMPSA í†µê³„ëŸ‰: ì „ì—­ì  ê³µê°„-RF ì—°ê´€ì„±
  n <- nrow(P)
  
  # Moran's Iì™€ ìœ ì‚¬í•œ í˜•íƒœë¡œ ì •ì˜
  P_centered <- P - mean(P)
  W_sum <- sum(W)
  
  GMPSA <- (n / W_sum) * sum(W * P_centered) / sum(P_centered^2)
  
  # ìˆœì—´ ê²€ì •
  GMPSA_perm <- numeric(n_perm)
  for (i in 1:n_perm) {
    perm_idx <- sample(1:n)
    P_perm <- P[perm_idx, perm_idx]
    P_perm_centered <- P_perm - mean(P_perm)
    GMPSA_perm[i] <- (n / W_sum) * sum(W * P_perm_centered) / sum(P_perm_centered^2)
  }
  
  # p-value ê³„ì‚°
  p_value <- (sum(abs(GMPSA_perm) >= abs(GMPSA)) + 1) / (n_perm + 1)
  
  # ê¸°ëŒ€ê°’ê³¼ ë¶„ì‚°
  E_GMPSA <- mean(GMPSA_perm)
  Var_GMPSA <- var(GMPSA_perm)
  z_score <- (GMPSA - E_GMPSA) / sqrt(Var_GMPSA)
  
  return(list(
    GMPSA = GMPSA,
    expected = E_GMPSA,
    variance = Var_GMPSA,
    z_score = z_score,
    p_value = p_value,
    permutations = GMPSA_perm
  ))
}

# === 2. ë‹¤ì¤‘ ìŠ¤ì¼€ì¼ MPSA ë¶„ì„ ===============================================

#' Multi-scale MPSA Analysis
#' 
#' @description ë‹¤ì–‘í•œ ê³µê°„ ìŠ¤ì¼€ì¼ì—ì„œ MPSA ë¶„ì„ ìˆ˜í–‰
#' @param P proximity matrix
#' @param coords spatial coordinates
#' @param scales vector of distance thresholds
compute_multiscale_MPSA <- function(P, coords, scales = c(1000, 2500, 5000, 10000)) {
  n <- nrow(P)
  results <- list()
  
  # ê° ìŠ¤ì¼€ì¼ë³„ë¡œ ë¶„ì„
  for (k in seq_along(scales)) {
    # ê±°ë¦¬ ê¸°ë°˜ ê°€ì¤‘ì¹˜ í–‰ë ¬ ìƒì„±
    dist_mat <- as.matrix(dist(coords))
    W_k <- (dist_mat <= scales[k]) * 1
    diag(W_k) <- 0
    
    # í–‰ í‘œì¤€í™”
    W_k_rs <- W_k / rowSums(W_k)
    W_k_rs[is.nan(W_k_rs)] <- 0
    
    # MPSA ê³„ì‚°
    mpsa_k <- rowSums(W_k_rs * P)
    
    # ìœ ì˜ì„± ê²€ì •
    sig_test <- compute_local_mpsa_significance_advanced(P, W_k_rs)
    
    results[[paste0("scale_", scales[k])]] <- list(
      scale = scales[k],
      W = W_k_rs,
      MPSA = mpsa_k,
      significance = sig_test,
      n_neighbors = rowSums(W_k)
    )
  }
  
  class(results) <- "multiscale_MPSA"
  return(results)
}

# === 3. ì‹œê³µê°„ MPSA (Spatio-temporal MPSA) ==================================

#' Spatio-temporal MPSA
#' 
#' @description ì‹œê³µê°„ ë°ì´í„°ì— ëŒ€í•œ MPSA ë¶„ì„
#' @param data_list list of data frames for each time point
#' @param spatial_weights spatial weight matrix
#' @param temporal_weights temporal weight matrix
compute_spatiotemporal_MPSA <- function(data_list, spatial_weights, 
                                      temporal_lag = 1, alpha = 0.5) {
  n_time <- length(data_list)
  n_space <- nrow(data_list[[1]])
  
  # ê° ì‹œì ë³„ proximity matrix ê³„ì‚°
  P_list <- list()
  for (t in 1:n_time) {
    rf_t <- randomForest(x = data_list[[t]], proximity = TRUE, ntree = 500)
    P_list[[t]] <- rf_t$proximity
  }
  
  # ì‹œê³µê°„ MPSA ê³„ì‚°
  ST_MPSA <- matrix(NA, nrow = n_space, ncol = n_time)
  
  for (t in 1:n_time) {
    # ê³µê°„ ì„±ë¶„
    spatial_component <- rowSums(spatial_weights * P_list[[t]])
    
    # ì‹œê°„ ì„±ë¶„ (ì´ì „ ì‹œì ë“¤ì˜ ì˜í–¥)
    temporal_component <- 0
    if (t > 1) {
      for (lag in 1:min(temporal_lag, t-1)) {
        weight <- (1 - alpha)^(lag - 1) * alpha
        temporal_component <- temporal_component + 
          weight * diag(P_list[[t]] %*% t(P_list[[t-lag]]))
      }
    }
    
    # ê²°í•©
    ST_MPSA[, t] <- spatial_component + temporal_component
  }
  
  return(list(
    ST_MPSA = ST_MPSA,
    P_list = P_list,
    spatial_component = spatial_component,
    temporal_component = temporal_component
  ))
}

# === 4. Bayesian MPSA =======================================================

#' Bayesian MPSA with Spatial Random Effects
#' 
#' @description ë² ì´ì§€ì•ˆ ì ‘ê·¼ë²•ì„ í†µí•œ MPSA ì¶”ì •
#' @param P proximity matrix
#' @param W spatial weight matrix
#' @param X covariates (optional)
#' @param n_iter MCMC iterations
compute_bayesian_MPSA <- function(P, W, X = NULL, n_iter = 10000, 
                                 burn_in = 2000, thin = 5) {
  n <- nrow(P)
  
  # MPSAë¥¼ ì¢…ì†ë³€ìˆ˜ë¡œ ì„¤ì •
  y <- rowSums(W * P)
  
  # ì„¤ê³„ í–‰ë ¬ êµ¬ì„±
  if (is.null(X)) {
    X <- matrix(1, n, 1)  # intercept only
  } else {
    X <- cbind(1, X)  # add intercept
  }
  
  # CAR ëª¨ë¸ ì„¤ì •ì„ ìœ„í•œ ì¤€ë¹„
  W_car <- W + t(W)  # symmetrize
  D <- diag(rowSums(W_car))
  Q <- D - W_car  # precision matrix for CAR
  
  # MCMC ì´ˆê¸°ê°’
  beta <- rep(0, ncol(X))
  tau2 <- 1  # error variance
  phi <- rep(0, n)  # spatial random effects
  rho <- 0.5  # spatial correlation parameter
  
  # MCMC ì €ì¥ì†Œ
  n_save <- (n_iter - burn_in) / thin
  beta_samples <- matrix(NA, n_save, ncol(X))
  tau2_samples <- numeric(n_save)
  phi_samples <- matrix(NA, n_save, n)
  rho_samples <- numeric(n_save)
  
  # MCMC ìƒ˜í”Œë§
  iter_save <- 0
  for (iter in 1:n_iter) {
    # Update beta
    V_beta <- solve(t(X) %*% X / tau2 + diag(0.001, ncol(X)))
    m_beta <- V_beta %*% (t(X) %*% (y - phi) / tau2)
    beta <- MASS::mvrnorm(1, m_beta, V_beta)
    
    # Update phi (spatial effects)
    Q_phi <- Q * rho / tau2 + diag(1e-6, n)
    V_phi <- solve(Q_phi)
    m_phi <- V_phi %*% ((y - X %*% beta) * rho / tau2)
    phi <- MASS::mvrnorm(1, m_phi, V_phi)
    
    # Update tau2
    a_tau <- n/2 + 0.1
    b_tau <- sum((y - X %*% beta - phi)^2) / 2 + 0.1
    tau2 <- 1 / rgamma(1, a_tau, b_tau)
    
    # Update rho (Metropolis step)
    rho_prop <- runif(1, max(0, rho - 0.1), min(1, rho + 0.1))
    log_ratio <- -0.5 * t(phi) %*% Q %*% phi * (rho_prop - rho) / tau2
    if (log(runif(1)) < log_ratio) {
      rho <- rho_prop
    }
    
    # ì €ì¥
    if (iter > burn_in && (iter - burn_in) %% thin == 0) {
      iter_save <- iter_save + 1
      beta_samples[iter_save, ] <- beta
      tau2_samples[iter_save] <- tau2
      phi_samples[iter_save, ] <- phi
      rho_samples[iter_save] <- rho
    }
  }
  
  # ì‚¬í›„ ìš”ì•½
  beta_post <- colMeans(beta_samples)
  phi_post <- colMeans(phi_samples)
  
  # Credible intervals
  beta_CI <- apply(beta_samples, 2, quantile, probs = c(0.025, 0.975))
  phi_CI <- apply(phi_samples, 2, quantile, probs = c(0.025, 0.975))
  
  # DIC ê³„ì‚°
  D_bar <- mean(-2 * dnorm(y, X %*% beta_post + phi_post, sqrt(mean(tau2_samples)), log = TRUE))
  D_theta <- -2 * sum(dnorm(y, X %*% beta_post + phi_post, sqrt(mean(tau2_samples)), log = TRUE))
  pD <- D_bar - D_theta
  DIC <- D_bar + pD
  
  return(list(
    beta = beta_post,
    beta_CI = beta_CI,
    phi = phi_post,
    phi_CI = phi_CI,
    tau2 = mean(tau2_samples),
    rho = mean(rho_samples),
    DIC = DIC,
    samples = list(
      beta = beta_samples,
      tau2 = tau2_samples,
      phi = phi_samples,
      rho = rho_samples
    )
  ))
}

# === 5. ë„¤íŠ¸ì›Œí¬ ê¸°ë°˜ MPSA ==================================================

#' Network-based MPSA Analysis
#' 
#' @description proximity matrixë¥¼ ë„¤íŠ¸ì›Œí¬ë¡œ ë³€í™˜í•˜ì—¬ ë¶„ì„
#' @param P proximity matrix
#' @param threshold proximity threshold for edge creation
compute_network_MPSA <- function(P, coords, threshold = 0.7) {
  n <- nrow(P)
  
  # Proximity ë„¤íŠ¸ì›Œí¬ êµ¬ì„±
  adj_matrix <- (P > threshold) * 1
  diag(adj_matrix) <- 0
  
  # igraph ê°ì²´ ìƒì„±
  g <- graph_from_adjacency_matrix(adj_matrix, mode = "undirected")
  
  # ë„¤íŠ¸ì›Œí¬ ì§€í‘œ ê³„ì‚°
  degree_cent <- degree(g)
  between_cent <- betweenness(g)
  eigen_cent <- eigen_centrality(g)$vector
  clustering <- transitivity(g, type = "local")
  
  # ê³µê°„ ë„¤íŠ¸ì›Œí¬ì™€ì˜ ë¹„êµ
  dist_mat <- as.matrix(dist(coords))
  spatial_adj <- (dist_mat < quantile(dist_mat[upper.tri(dist_mat)], 0.1)) * 1
  diag(spatial_adj) <- 0
  g_spatial <- graph_from_adjacency_matrix(spatial_adj, mode = "undirected")
  
  # ë„¤íŠ¸ì›Œí¬ ìœ ì‚¬ì„± ì¸¡ì •
  # Jaccard similarity of edges
  edges_rf <- which(adj_matrix == 1, arr.ind = TRUE)
  edges_spatial <- which(spatial_adj == 1, arr.ind = TRUE)
  
  edges_rf_set <- paste(edges_rf[,1], edges_rf[,2], sep = "-")
  edges_spatial_set <- paste(edges_spatial[,1], edges_spatial[,2], sep = "-")
  
  jaccard <- length(intersect(edges_rf_set, edges_spatial_set)) / 
             length(union(edges_rf_set, edges_spatial_set))
  
  # Community detection
  communities <- cluster_louvain(g)
  
  return(list(
    graph = g,
    degree = degree_cent,
    betweenness = between_cent,
    eigenvector = eigen_cent,
    clustering = clustering,
    communities = membership(communities),
    spatial_similarity = jaccard,
    adj_matrix = adj_matrix
  ))
}

# === 6. ê³ ê¸‰ ìœ ì˜ì„± ê²€ì • ====================================================

#' Advanced Significance Testing for MPSA
#' 
#' @description ë‹¤ì–‘í•œ null hypothesisì— ëŒ€í•œ ê²€ì •
compute_local_mpsa_significance_advanced <- function(P, W, n_perm = 999, 
                                                  alpha = 0.05,
                                                  null_model = "CSR") {
  n <- nrow(P)
  observed_mpsa <- rowSums(W * P)
  
  # Null modelì— ë”°ë¥¸ ìˆœì—´ ìƒì„±
  perm_matrix <- matrix(NA, nrow = n, ncol = n_perm)
  
  if (null_model == "CSR") {
    # Complete Spatial Randomness
    for (b in 1:n_perm) {
      perm_idx <- sample(1:n)
      P_perm <- P[perm_idx, perm_idx]
      perm_matrix[, b] <- rowSums(W * P_perm)
    }
  } else if (null_model == "CAR") {
    # Conditional Autoregressive null
    for (b in 1:n_perm) {
      # CAR í”„ë¡œì„¸ìŠ¤ì—ì„œ ìƒ˜í”Œë§
      car_sample <- MASS::mvrnorm(1, rep(0, n), solve(diag(n) - 0.5 * W))
      perm_order <- order(car_sample)
      P_perm <- P[perm_order, perm_order]
      perm_matrix[, b] <- rowSums(W * P_perm)
    }
  } else if (null_model == "toroidal") {
    # Toroidal shift (edge effect ì œê±°)
    for (b in 1:n_perm) {
      shift <- sample(1:n, 1)
      perm_idx <- ((1:n + shift - 1) %% n) + 1
      P_perm <- P[perm_idx, perm_idx]
      perm_matrix[, b] <- rowSums(W * P_perm)
    }
  }
  
  # Enhanced p-value calculation
  # 1. Standard p-value
  p_greater <- rowSums(perm_matrix >= observed_mpsa) / n_perm
  p_less <- rowSums(perm_matrix <= observed_mpsa) / n_perm
  p_two_sided <- 2 * pmin(p_greater, p_less)
  
  # 2. Exact p-value using empirical distribution
  p_exact <- sapply(1:n, function(i) {
    ecdf_i <- ecdf(perm_matrix[i, ])
    1 - ecdf_i(observed_mpsa[i]) + ecdf_i(observed_mpsa[i] - .Machine$double.eps)
  })
  
  # 3. Multiple testing correction (ì—¬ëŸ¬ ë°©ë²• ì œê³µ)
  p_bonferroni <- p.adjust(p_two_sided, method = "bonferroni")
  p_fdr <- p.adjust(p_two_sided, method = "fdr")
  p_BY <- p.adjust(p_two_sided, method = "BY")  # Benjamini-Yekutieli
  
  # Effect size calculation
  mean_perm <- rowMeans(perm_matrix)
  sd_perm <- apply(perm_matrix, 1, sd)
  effect_size <- (observed_mpsa - mean_perm) / sd_perm
  
  # Classification with confidence
  classification <- cut(effect_size,
                      breaks = c(-Inf, -2, -1, 1, 2, Inf),
                      labels = c("Strong Coldspot", "Coldspot", 
                               "Not Significant", "Hotspot", "Strong Hotspot"))
  
  # Confidence scores
  confidence <- 1 - p_fdr
  
  return(data.frame(
    MPSA = observed_mpsa,
    expected = mean_perm,
    variance = sd_perm^2,
    z_score = effect_size,
    p_value = p_two_sided,
    p_exact = p_exact,
    p_bonferroni = p_bonferroni,
    p_fdr = p_fdr,
    p_BY = p_BY,
    category = classification,
    confidence = confidence,
    n_neighbors = rowSums(W > 0)
  ))
}

# === 7. Variogram ë¶„ì„ ======================================================

#' MPSA Variogram Analysis
#' 
#' @description MPSAì˜ ê³µê°„ì  ì˜ì¡´ì„± êµ¬ì¡° ë¶„ì„
compute_MPSA_variogram <- function(mpsa_values, coords, cutoff = NULL, width = NULL) {
  # ë°ì´í„° ì¤€ë¹„
  mpsa_df <- data.frame(
    mpsa = mpsa_values,
    x = coords[, 1],
    y = coords[, 2]
  )
  
  # sp ê°ì²´ë¡œ ë³€í™˜
  coordinates(mpsa_df) <- ~x + y
  
  # Empirical variogram
  if (is.null(cutoff)) {
    cutoff <- max(dist(coords)) / 3
  }
  if (is.null(width)) {
    width <- cutoff / 15
  }
  
  v_emp <- variogram(mpsa ~ 1, mpsa_df, cutoff = cutoff, width = width)
  
  # Fit variogram models
  v_fit_exp <- fit.variogram(v_emp, vgm("Exp"))
  v_fit_sph <- fit.variogram(v_emp, vgm("Sph"))
  v_fit_gau <- fit.variogram(v_emp, vgm("Gau"))
  v_fit_mat <- fit.variogram(v_emp, vgm("Mat"))
  
  # Model selection
  models <- list(
    Exponential = v_fit_exp,
    Spherical = v_fit_sph,
    Gaussian = v_fit_gau,
    Matern = v_fit_mat
  )
  
  # Calculate fit statistics
  fit_stats <- lapply(models, function(model) {
    pred <- variogramLine(model, dist_vector = v_emp$dist)
    obs <- v_emp$gamma
    pred_at_obs <- approx(pred$dist, pred$gamma, v_emp$dist)$y
    
    rmse <- sqrt(mean((obs - pred_at_obs)^2, na.rm = TRUE))
    r2 <- cor(obs, pred_at_obs, use = "complete.obs")^2
    
    return(c(RMSE = rmse, R2 = r2))
  })
  
  # Best model
  best_model_idx <- which.min(sapply(fit_stats, function(x) x["RMSE"]))
  best_model <- models[[best_model_idx]]
  
  return(list(
    empirical = v_emp,
    models = models,
    fit_stats = fit_stats,
    best_model = best_model,
    best_model_name = names(models)[best_model_idx],
    range = best_model$range[2],  # spatial range
    sill = sum(best_model$psill),  # total sill
    nugget = best_model$psill[1]   # nugget effect
  ))
}

# === 8. ê³ ê¸‰ ì‹œê°í™” í•¨ìˆ˜ë“¤ ==================================================

#' Create comprehensive MPSA visualization
#' 
#' @description MPSA ë¶„ì„ ê²°ê³¼ì˜ ì¢…í•©ì  ì‹œê°í™”
visualize_MPSA_comprehensive <- function(franklin, mpsa_result, network_result,
                                       variogram_result, multiscale_result) {
  
  # 1. ê¸°ë³¸ MPSA ë§µ
  p1 <- tm_shape(franklin) +
    tm_polygons("MPSA", 
                style = "jenks",
                palette = "RdBu",
                midpoint = 0,
                title = "MPSA Values") +
    tm_layout(title = "Local Indicators of Random Forest",
              frame = FALSE)
  
  # 2. ìœ ì˜ì„± ë§µ
  franklin$category <- mpsa_result$category
  p2 <- tm_shape(franklin) +
    tm_polygons("category",
                palette = c("Strong Coldspot" = "#2166AC",
                          "Coldspot" = "#67A9CF", 
                          "Not Significant" = "#F7F7F7",
                          "Hotspot" = "#EF8A62",
                          "Strong Hotspot" = "#B2182B"),
                title = "Significance") +
    tm_layout(title = "MPSA Significance Test",
              frame = FALSE)
  
  # 3. ë„¤íŠ¸ì›Œí¬ ì‹œê°í™”
  # Extract coordinates for network plot
  coords_df <- st_coordinates(st_centroid(franklin))
  
  # Create edge list from adjacency matrix
  edges <- which(network_result$adj_matrix == 1, arr.ind = TRUE)
  edge_df <- data.frame(
    from = edges[, 1],
    to = edges[, 2]
  )
  
  # Create network layout
  g <- network_result$graph
  layout <- layout_with_fr(g, coords = coords_df)
  
  p3 <- ggraph(g, layout = layout) +
    geom_edge_link(alpha = 0.3) +
    geom_node_point(aes(size = network_result$degree,
                       color = network_result$clustering)) +
    scale_color_viridis(name = "Clustering\nCoefficient") +
    scale_size_continuous(name = "Degree") +
    theme_graph() +
    labs(title = "Proximity Network Structure")
  
  # 4. Variogram plot
  v_df <- variogram_result$empirical
  v_model <- variogramLine(variogram_result$best_model, 
                          maxdist = max(v_df$dist))
  
  p4 <- ggplot() +
    geom_point(data = v_df, aes(x = dist, y = gamma), size = 3) +
    geom_line(data = v_model, aes(x = dist, y = gamma), 
              color = "red", size = 1) +
    labs(x = "Distance", y = "Semivariance",
         title = paste("MPSA Variogram -", variogram_result$best_model_name),
         subtitle = sprintf("Range: %.0f, Sill: %.3f, Nugget: %.3f",
                          variogram_result$range,
                          variogram_result$sill,
                          variogram_result$nugget)) +
    theme_minimal()
  
  # 5. Multi-scale comparison
  scales <- names(multiscale_result)
  scale_df <- data.frame(
    scale = numeric(),
    mean_mpsa = numeric(),
    sd_mpsa = numeric(),
    moran_i = numeric()
  )
  
  for (s in scales) {
    scale_val <- multiscale_result[[s]]$scale
    mpsa_vals <- multiscale_result[[s]]$MPSA
    W <- multiscale_result[[s]]$W
    
    # Calculate Moran's I for comparison
    moran_test <- moran.test(mpsa_vals, mat2listw(W))
    
    scale_df <- rbind(scale_df, data.frame(
      scale = scale_val,
      mean_mpsa = mean(mpsa_vals),
      sd_mpsa = sd(mpsa_vals),
      moran_i = moran_test$statistic
    ))
  }
  
  p5 <- ggplot(scale_df, aes(x = scale)) +
    geom_line(aes(y = mean_mpsa), color = "blue", size = 1) +
    geom_ribbon(aes(ymin = mean_mpsa - sd_mpsa, 
                   ymax = mean_mpsa + sd_mpsa),
                alpha = 0.3, fill = "blue") +
    labs(x = "Spatial Scale (meters)", y = "Mean MPSA Â± SD",
         title = "Multi-scale MPSA Analysis") +
    theme_minimal()
  
  # 6. Effect size distribution
  p6 <- ggplot(mpsa_result, aes(x = z_score, fill = category)) +
    geom_histogram(bins = 30, alpha = 0.7) +
    geom_vline(xintercept = c(-2, -1, 1, 2), 
               linetype = "dashed", alpha = 0.5) +
    scale_fill_manual(values = c("Strong Coldspot" = "#2166AC",
                               "Coldspot" = "#67A9CF", 
                               "Not Significant" = "#F7F7F7",
                               "Hotspot" = "#EF8A62",
                               "Strong Hotspot" = "#B2182B")) +
    labs(x = "Effect Size (Z-score)", y = "Frequency",
         title = "MPSA Effect Size Distribution") +
    theme_minimal()
  
  # Combine all plots
  combined_plot <- (p1 | p2) / (p3 | p4) / (p5 | p6)
  
  return(list(
    individual_plots = list(
      mpsa_map = p1,
      significance_map = p2,
      network = p3,
      variogram = p4,
      multiscale = p5,
      effect_sizes = p6
    ),
    combined = combined_plot
  ))
}

# === 9. ì§„ë‹¨ ë° ê²€ì¦ ë„êµ¬ ===================================================

#' MPSA Diagnostics
#' 
#' @description MPSA ë¶„ì„ì˜ ê°€ì • ê²€ì¦ ë° ì§„ë‹¨
diagnose_MPSA <- function(P, W, mpsa_values, coords) {
  n <- nrow(P)
  diagnostics <- list()
  
  # 1. Spatial autocorrelation of residuals
  mpsa_expected <- rowMeans(replicate(100, {
    perm_idx <- sample(1:n)
    rowSums(W * P[perm_idx, perm_idx])
  }))
  residuals <- mpsa_values - mpsa_expected
  
  moran_resid <- moran.test(residuals, mat2listw(W))
  diagnostics$moran_residuals <- moran_resid
  
  # 2. Proximity matrix properties
  # Symmetry check
  diagnostics$proximity_symmetric <- all.equal(P, t(P))
  
  # Distribution of proximities
  prox_upper <- P[upper.tri(P)]
  diagnostics$proximity_stats <- summary(prox_upper)
  
  # 3. Spatial weight matrix diagnostics
  diagnostics$W_connected <- is.connected(graph_from_adjacency_matrix(W))
  diagnostics$W_row_sums <- summary(rowSums(W))
  
  # 4. Influence diagnostics
  # Leave-one-out analysis
  loo_mpsa <- matrix(NA, n, n)
  for (i in 1:n) {
    W_loo <- W[-i, -i]
    P_loo <- P[-i, -i]
    loo_mpsa[-i, i] <- rowSums(W_loo * P_loo)
  }
  
  # Cook's distance equivalent
  influence <- colMeans(abs(loo_mpsa - mpsa_values[-1]), na.rm = TRUE)
  diagnostics$influence <- influence
  diagnostics$influential_units <- which(influence > mean(influence) + 2*sd(influence))
  
  # 5. Bootstrap confidence intervals
  boot_mpsa <- replicate(1000, {
    boot_idx <- sample(1:n, replace = TRUE)
    P_boot <- P[boot_idx, boot_idx]
    W_boot <- W[boot_idx, boot_idx]
    rowSums(W_boot * P_boot)
  })
  
  diagnostics$bootstrap_CI <- t(apply(boot_mpsa, 1, quantile, 
                                     probs = c(0.025, 0.975)))
  
  return(diagnostics)
}

# === 10. ë³´ê³ ì„œ ìƒì„± í•¨ìˆ˜ ===================================================

#' Generate MPSA Analysis Report
#' 
#' @description ë¶„ì„ ê²°ê³¼ë¥¼ ì¢…í•©í•œ ë³´ê³ ì„œ ìƒì„±
generate_MPSA_report <- function(results, output_dir = "output/MPSA_analysis") {
  if (!dir.exists(output_dir)) {
    dir.create(output_dir, recursive = TRUE)
  }
  
  # 1. Summary statistics
  summary_stats <- data.frame(
    Metric = c("Global IRF", "Mean Local IRF", "SD Local IRF",
               "% Hotspots", "% Coldspots", "Spatial Range",
               "Network Density", "Spatial-RF Similarity"),
    Value = c(
      results$GMPSA$GMPSA,
      mean(results$mpsa_result$MPSA),
      sd(results$mpsa_result$MPSA),
      mean(results$mpsa_result$category %in% c("Hotspot", "Strong Hotspot")) * 100,
      mean(results$mpsa_result$category %in% c("Coldspot", "Strong Coldspot")) * 100,
      results$variogram$range,
      edge_density(results$network$graph),
      results$network$spatial_similarity
    )
  )
  
  write.csv(summary_stats, file.path(output_dir, "summary_statistics.csv"))
  
  # 2. Detailed results
  write.csv(results$mpsa_result, file.path(output_dir, "mpsa_detailed_results.csv"))
  
  # 3. Save plots
  ggsave(file.path(output_dir, "comprehensive_visualization.png"), 
         results$plots$combined, width = 16, height = 20, dpi = 300)
  
  # 4. Generate markdown report
  report_content <- sprintf("
# MPSA Analysis Report
Generated: %s

## Executive Summary
- Global Indicator of Random Forest (GIRF): %.4f (p = %.4f)
- Identified %d hotspots and %d coldspots
- Spatial range of dependence: %.0f meters
- Best-fitting variogram model: %s

## Key Findings
1. **Spatial Clustering**: The analysis reveals significant spatial clustering with %.1f%% of units classified as hotspots or coldspots.
2. **Network Structure**: The proximity network shows %.1f%% similarity with the spatial network.
3. **Scale Dependency**: Multi-scale analysis indicates strongest patterns at %.0f meter scale.

## Methodological Notes
- Random Forest with %d trees
- Permutation tests with %d iterations
- Multiple testing correction: FDR (Benjamini-Hochberg)
- Bayesian inference with DIC = %.2f
",
    Sys.Date(),
    results$GMPSA$GMPSA, results$GMPSA$p_value,
    sum(results$mpsa_result$category %in% c("Hotspot", "Strong Hotspot")),
    sum(results$mpsa_result$category %in% c("Coldspot", "Strong Coldspot")),
    results$variogram$range,
    results$variogram$best_model_name,
    mean(results$mpsa_result$category != "Not Significant") * 100,
    results$network$spatial_similarity * 100,
    results$multiscale$optimal_scale,
    500,  # ntree
    999,  # n_perm
    results$bayesian$DIC
  )
  
  writeLines(report_content, file.path(output_dir, "analysis_report.md"))
  
  print(paste("Report generated in:", output_dir))
}

# === 9. í†µí•© ë¶„ì„ í•¨ìˆ˜ ======================================================

#' Run Complete MPSA Analysis
#' 
#' @description ëª¨ë“  MPSA ê¸°ëŠ¥ì„ í¬í•¨í•œ í†µí•© ë¶„ì„
#' @param data sf ê°ì²´ (ê³µê°„ ë°ì´í„°)
#' @param ntree Random Forest íŠ¸ë¦¬ ìˆ˜
#' @param n_perm ìˆœì—´ ê²€ì • íšŸìˆ˜
#' @param scales ë‹¤ì¤‘ ìŠ¤ì¼€ì¼ ë¶„ì„ìš© ê±°ë¦¬ ë²¡í„°
#' @param include_advanced ê³ ê¸‰ ë¶„ì„ í¬í•¨ ì—¬ë¶€
#' @return ì™„ì „í•œ MPSA ë¶„ì„ ê²°ê³¼
run_complete_MPSA_analysis <- function(data, ntree = 500, n_perm = 999, 
                                     scales = c(1000, 2500, 5000, 10000),
                                     include_advanced = TRUE) {
  
  cat("=== Complete MPSA Analysis ===\n")
  cat(sprintf("Data: %d spatial units\n", nrow(data)))
  
  # 1. ë°ì´í„° ì¤€ë¹„
  rf_data <- data |> 
    st_drop_geometry() |> 
    select(where(is.numeric)) |>
    select(-any_of(c("GEOID", "NAME")))  # ID ì»¬ëŸ¼ ì œì™¸
  
  coords <- st_coordinates(st_centroid(data))
  
  cat(sprintf("Variables: %d numeric variables\n", ncol(rf_data)))
  
  # 2. Random Forest ë° Proximity ê³„ì‚°
  cat("\nComputing Random Forest proximity matrix...\n")
  set.seed(42)
  rf_model <- randomForest(
    x = rf_data,
    proximity = TRUE,
    ntree = ntree,
    importance = TRUE
  )
  
  P <- rf_model$proximity
  
  # 3. ê³µê°„ ê°€ì¤‘ì¹˜ í–‰ë ¬
  cat("Creating spatial weight matrices...\n")
  nb <- poly2nb(data, queen = TRUE)
  W_binary <- nb2mat(nb, style = "B", zero.policy = TRUE)
  W_rs <- nb2mat(nb, style = "W", zero.policy = TRUE)
  
  # 4. Global IRF ê³„ì‚°
  cat("Computing Global IRF...\n")
  girf_result <- compute_GMPSA(P, W_binary, n_perm = n_perm)
  cat(sprintf("GMPSA = %.4f (p = %.4f)\n", girf_result$GMPSA, girf_result$p_value))
  
  # 5. Local IRF ê³„ì‚° ë° ìœ ì˜ì„± ê²€ì •
  cat("Computing Local IRF with significance testing...\n")
  mpsa_result <- compute_local_mpsa_significance_advanced(
    P, W_rs, n_perm = n_perm, null_model = "CSR"
  )
  
  # ê²°ê³¼ ìš”ì•½
  n_hotspots <- sum(mpsa_result$category %in% c("Hotspot", "Strong Hotspot"))
  n_coldspots <- sum(mpsa_result$category %in% c("Coldspot", "Strong Coldspot"))
  cat(sprintf("Results: %d hotspots, %d coldspots, %d not significant\n",
              n_hotspots, n_coldspots, 
              nrow(data) - n_hotspots - n_coldspots))
  
  # 6. ë°ì´í„°ì— ê²°ê³¼ ì¶”ê°€
  data$MPSA <- mpsa_result$MPSA
  data$MPSA_category <- mpsa_result$category
  data$MPSA_pvalue <- mpsa_result$p_fdr
  data$MPSA_zscore <- mpsa_result$z_score
  
  # ê¸°ë³¸ ê²°ê³¼
  basic_results <- list(
    data = data,
    rf_model = rf_model,
    proximity_matrix = P,
    spatial_weights = list(binary = W_binary, row_std = W_rs),
    GMPSA = girf_result,
    MPSA = mpsa_result,
    coords = coords
  )
  
  # 7. ê³ ê¸‰ ë¶„ì„ (ì„ íƒì )
  if (include_advanced) {
    cat("\nRunning advanced analyses...\n")
    
    # Variogram ë¶„ì„
    cat("Computing variogram...\n")
    variogram_result <- compute_MPSA_variogram(
      mpsa_result$MPSA, coords
    )
    
    # ë‹¤ì¤‘ ìŠ¤ì¼€ì¼ ë¶„ì„
    cat("Multi-scale analysis...\n")
    multiscale_result <- compute_multiscale_MPSA(
      P, coords, scales = scales
    )
    
    # ë„¤íŠ¸ì›Œí¬ ë¶„ì„
    cat("Network analysis...\n")
    network_result <- analyze_proximity_network(
      P, coords, threshold = 0.6
    )
    
    # ê³ ê¸‰ ê²°ê³¼ ì¶”ê°€
    basic_results$variogram <- variogram_result
    basic_results$multiscale <- multiscale_result
    basic_results$network <- network_result
    
    # ì¢…í•© ì‹œê°í™”
    cat("Creating comprehensive visualization...\n")
    tryCatch({
      viz_result <- visualize_MPSA_comprehensive(
        data, mpsa_result, network_result, 
        variogram_result, multiscale_result
      )
      basic_results$plots <- viz_result
    }, error = function(e) {
      cat("Visualization error (continuing without plots):", e$message, "\n")
    })
  }
  
  cat("\n=== Analysis Complete ===\n")
  return(basic_results)
}

# === ë©”ì¸ ì‹¤í–‰ ì˜ˆì œ ==========================================================

# ì´ ë¶€ë¶„ì€ ì£¼ì„ ì²˜ë¦¬ë˜ì–´ ìˆìœ¼ë©°, ì‹¤ì œ ì‚¬ìš© ì‹œ ì£¼ì„ì„ í•´ì œí•˜ì„¸ìš”
# franklin <- read_rds("data/franklin.rds")
# 
# # ì „ì²´ ë¶„ì„ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
# results <- run_complete_MPSA_analysis(franklin)
# 
# # ë³´ê³ ì„œ ìƒì„±
# generate_MPSA_report(results) 

# === ì‹¤í–‰ ì˜ˆì‹œ ë° ì‹œê°í™” =====================================================

# === 1. ê³ ê¸‰ MPSA ë°©ë²•ë¡  ì‹¤í–‰ ===
# 
# # ë°ì´í„° ë¡œë“œ
# franklin <- readRDS("data/franklin.rds")
# numeric_data <- franklin %>% 
#   st_drop_geometry() %>% 
#   select(where(is.numeric)) %>%
#   select(-main_industry)
# 
# # ê³µê°„ ê°€ì¤‘ì¹˜ í–‰ë ¬ ìƒì„±
# coords <- st_coordinates(st_centroid(franklin))
# nb <- poly2nb(franklin, queen = TRUE)
# W_matrix <- nb2mat(nb, style = "W", zero.policy = TRUE)
# 
# # ë² ì´ì§€ì•ˆ MPSA ì‹¤í–‰
# bayesian_results <- compute_bayesian_MPSA(numeric_data, W_matrix)
# print("ë² ì´ì§€ì•ˆ MPSA ì™„ë£Œ")
# 
# # ì ì‘í˜• MPSA ì‹¤í–‰  
# adaptive_results <- compute_adaptive_MPSA(numeric_data, W_matrix)
# print("ì ì‘í˜• MPSA ì™„ë£Œ")

# === 2. ê³ ê¸‰ ë°©ë²•ë¡  ë¹„êµ ì‹œê°í™” ===
# 
# # ê¸°ë³¸ MPSAì™€ ê³ ê¸‰ ë°©ë²•ë¡  ë¹„êµ
# library(ggplot2)
# library(patchwork)
# 
# # ê¸°ë³¸ MPSA ê³„ì‚°
# rf_basic <- randomForest(numeric_data, proximity = TRUE, ntree = 500)
# mpsa_basic <- rowSums(W_matrix * rf_basic$proximity)
# 
# # ë¹„êµ ë°ì´í„° ì¤€ë¹„
# comparison_df <- data.frame(
#   region_id = 1:length(mpsa_basic),
#   basic_mpsa = mpsa_basic,
#   bayesian_mpsa = bayesian_results$posterior_mean,
#   adaptive_mpsa = adaptive_results$adapted_mpsa
# )
# 
# # ë°©ë²•ë¡ ë³„ ë¶„í¬ ë¹„êµ
# methods_long <- comparison_df %>%
#   gather(method, value, -region_id) %>%
#   mutate(method = factor(method, 
#                         levels = c("basic_mpsa", "bayesian_mpsa", "adaptive_mpsa"),
#                         labels = c("Basic MPSA", "Bayesian MPSA", "Adaptive MPSA")))
# 
# p1 <- ggplot(methods_long, aes(x = value, fill = method)) +
#   geom_histogram(bins = 30, alpha = 0.7, position = "identity") +
#   facet_wrap(~method, scales = "free_y", ncol = 1) +
#   labs(title = "Distribution Comparison of MPSA Methods",
#        x = "MPSA Value", y = "Frequency") +
#   theme_minimal() +
#   theme(legend.position = "none")

# === 3. ë² ì´ì§€ì•ˆ MPSA ìƒì„¸ ì‹œê°í™” ===
# 
# # ì‚¬í›„ ë¶„í¬ ì‹œê°í™”
# posterior_samples <- bayesian_results$posterior_samples
# 
# # ì‚¬í›„ ë¶„í¬ íˆìŠ¤í† ê·¸ë¨ (ì²˜ìŒ ëª‡ ê°œ ì§€ì—­)
# sample_regions <- 1:6
# posterior_df <- data.frame()
# 
# for (region in sample_regions) {
#   region_samples <- data.frame(
#     region_id = paste("Region", region),
#     posterior_value = posterior_samples[region, ]
#   )
#   posterior_df <- rbind(posterior_df, region_samples)
# }
# 
# p2 <- ggplot(posterior_df, aes(x = posterior_value)) +
#   geom_histogram(bins = 50, alpha = 0.7, fill = "steelblue") +
#   facet_wrap(~region_id, scales = "free", ncol = 3) +
#   labs(title = "Posterior Distributions for Selected Regions",
#        x = "Posterior MPSA", y = "Frequency") +
#   theme_minimal()

# === 4. ì ì‘í˜• MPSA ìƒì„¸ ë¶„ì„ ===
# 
# # ì ì‘í˜• ê°€ì¤‘ì¹˜ ì‹œê°í™”
# adaptive_weights <- adaptive_results$adaptive_weights
# 
# # ê°€ì¤‘ì¹˜ íˆíŠ¸ë§µ (ìƒìœ„ 20x20 ë¶€ë¶„)
# library(reshape2)
# weight_subset <- adaptive_weights[1:20, 1:20]
# weight_melted <- melt(weight_subset)
# 
# p4 <- ggplot(weight_melted, aes(x = Var1, y = Var2, fill = value)) +
#   geom_tile() +
#   scale_fill_gradient2(low = "blue", mid = "white", high = "red", 
#                        midpoint = 0, name = "Weight") +
#   labs(title = "Adaptive Spatial Weights (First 20x20)",
#        x = "Region", y = "Region") +
#   theme_minimal() +
#   theme(axis.text = element_text(size = 8))
# 
# # ì ì‘ì„± ì§€í‘œ ì‹œê°í™”
# adaptivity_scores <- adaptive_results$adaptivity_scores
# 
# p5 <- ggplot(data.frame(region_id = 1:length(adaptivity_scores), 
#                        adaptivity = adaptivity_scores), 
#              aes(x = region_id, y = adaptivity)) +
#   geom_line(color = "darkgreen", size = 0.8) +
#   geom_point(color = "orange", size = 0.8) +
#   labs(title = "Regional Adaptivity Scores",
#        subtitle = "Higher scores indicate more adaptive spatial relationships",
#        x = "Region ID", y = "Adaptivity Score") +
#   theme_minimal()

# === 5. ë°©ë²•ë¡  ê°„ ìƒê´€ê´€ê³„ ë¶„ì„ ===
# 
# # ìƒê´€ê´€ê³„ ë§¤íŠ¸ë¦­ìŠ¤
# cor_matrix <- cor(comparison_df[, -1], use = "complete.obs")
# 
# # ìƒê´€ê´€ê³„ íˆíŠ¸ë§µ
# library(corrplot)
# png("output/advanced_mpsa/correlation_heatmap.png", width = 600, height = 600)
# corrplot(cor_matrix, method = "color", type = "upper", 
#          order = "hclust", tl.cex = 1, tl.col = "black",
#          title = "Correlation Between MPSA Methods")
# dev.off()
# 
# # ì‚°ì ë„ ë§¤íŠ¸ë¦­ìŠ¤
# library(GGally)
# p6 <- ggpairs(comparison_df[, -1], 
#               columnLabels = c("Basic MPSA", "Bayesian MPSA", "Adaptive MPSA"),
#               title = "Pairwise Comparisons of MPSA Methods") +
#   theme_minimal()

# === 6. ê³µê°„ íŒ¨í„´ ë¹„êµ ì§€ë„ ===
# 
# # ê³µê°„ ë°ì´í„°ì— ê²°ê³¼ ì¶”ê°€
# franklin_advanced <- franklin
# franklin_advanced$basic_mpsa <- mpsa_basic
# franklin_advanced$bayesian_mpsa <- bayesian_results$posterior_mean
# franklin_advanced$adaptive_mpsa <- adaptive_results$adapted_mpsa
# franklin_advanced$uncertainty <- bayesian_results$posterior_sd
# 
# # ë²”ì£¼í™”
# franklin_advanced$basic_category <- cut(mpsa_basic, breaks = 5, labels = c("Very Low", "Low", "Medium", "High", "Very High"))
# franklin_advanced$bayesian_category <- cut(bayesian_results$posterior_mean, breaks = 5, labels = c("Very Low", "Low", "Medium", "High", "Very High"))
# franklin_advanced$adaptive_category <- cut(adaptive_results$adapted_mpsa, breaks = 5, labels = c("Very Low", "Low", "Medium", "High", "Very High"))
# 
# # tmap ì§€ë„ë“¤
# library(tmap)
# 
# # ê¸°ë³¸ MPSA ì§€ë„
# map1 <- tm_shape(franklin_advanced) +
#   tm_fill("basic_category", title = "Basic MPSA", palette = "YlOrRd") +
#   tm_borders(alpha = 0.3) +
#   tm_layout(title = "Basic MPSA")
# 
# # ë² ì´ì§€ì•ˆ MPSA ì§€ë„
# map2 <- tm_shape(franklin_advanced) +
#   tm_fill("bayesian_category", title = "Bayesian MPSA", palette = "YlOrRd") +
#   tm_borders(alpha = 0.3) +
#   tm_layout(title = "Bayesian MPSA")
# 
# # ì ì‘í˜• MPSA ì§€ë„
# map3 <- tm_shape(franklin_advanced) +
#   tm_fill("adaptive_category", title = "Adaptive MPSA", palette = "YlOrRd") +
#   tm_borders(alpha = 0.3) +
#   tm_layout(title = "Adaptive MPSA")
# 
# # ë¶ˆí™•ì‹¤ì„± ì§€ë„
# map4 <- tm_shape(franklin_advanced) +
#   tm_fill("uncertainty", title = "Uncertainty", palette = "Blues") +
#   tm_borders(alpha = 0.3) +
#   tm_layout(title = "Bayesian Uncertainty")
# 
# # ê²°í•© ì§€ë„
# combined_maps <- tmap_arrange(map1, map2, map3, map4, ncol = 2)

# === 7. ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ===
# 
# # ê³„ì‚° ì‹œê°„ ë¹„êµ
# execution_times <- data.frame(
#   Method = character(),
#   Time_seconds = numeric(),
#   stringsAsFactors = FALSE
# )
# 
# # ê¸°ë³¸ MPSA ì‹œê°„
# start_time <- Sys.time()
# rf_temp <- randomForest(numeric_data, proximity = TRUE, ntree = 500)
# basic_time <- as.numeric(Sys.time() - start_time)
# execution_times <- rbind(execution_times, data.frame(Method = "Basic MPSA", Time_seconds = basic_time))
# 
# # ë² ì´ì§€ì•ˆ MPSA ì‹œê°„
# start_time <- Sys.time()
# bayesian_temp <- compute_bayesian_MPSA(numeric_data[1:50, ], W_matrix[1:50, 1:50], n_iter = 100)
# bayesian_time <- as.numeric(Sys.time() - start_time)
# execution_times <- rbind(execution_times, data.frame(Method = "Bayesian MPSA (50 regions)", Time_seconds = bayesian_time))
# 
# # ì ì‘í˜• MPSA ì‹œê°„
# start_time <- Sys.time()
# adaptive_temp <- compute_adaptive_MPSA(numeric_data[1:50, ], W_matrix[1:50, 1:50])
# adaptive_time <- as.numeric(Sys.time() - start_time)
# execution_times <- rbind(execution_times, data.frame(Method = "Adaptive MPSA (50 regions)", Time_seconds = adaptive_time))
# 
# # ì‹œê°„ ë¹„êµ ì‹œê°í™”
# p7 <- ggplot(execution_times, aes(x = reorder(Method, Time_seconds), y = Time_seconds)) +
#   geom_col(fill = "lightcoral", alpha = 0.8) +
#   coord_flip() +
#   labs(title = "Execution Time Comparison",
#        x = "Method", y = "Time (seconds)") +
#   theme_minimal()

# === 8. í†µê³„ì  ë¹„êµ ë¶„ì„ ===
# 
# # ë°©ë²•ë¡ ë³„ ê¸°ìˆ í†µê³„ëŸ‰
# stats_comparison <- data.frame(
#   Method = c("Basic MPSA", "Bayesian MPSA", "Adaptive MPSA"),
#   Mean = c(mean(mpsa_basic), mean(bayesian_results$posterior_mean), mean(adaptive_results$adapted_mpsa)),
#   SD = c(sd(mpsa_basic), sd(bayesian_results$posterior_mean), sd(adaptive_results$adapted_mpsa)),
#   Min = c(min(mpsa_basic), min(bayesian_results$posterior_mean), min(adaptive_results$adapted_mpsa)),
#   Max = c(max(mpsa_basic), max(bayesian_results$posterior_mean), max(adaptive_results$adapted_mpsa)),
#   Median = c(median(mpsa_basic), median(bayesian_results$posterior_mean), median(adaptive_results$adapted_mpsa))
# )
# 
# # ì°¨ì´ì  ë¶„ì„
# differences_df <- data.frame(
#   region_id = 1:length(mpsa_basic),
#   bayesian_vs_basic = bayesian_results$posterior_mean - mpsa_basic,
#   adaptive_vs_basic = adaptive_results$adapted_mpsa - mpsa_basic,
#   adaptive_vs_bayesian = adaptive_results$adapted_mpsa - bayesian_results$posterior_mean
# )
# 
# # ì°¨ì´ì  ì‹œê°í™”
# differences_long <- differences_df %>%
#   gather(comparison, difference, -region_id) %>%
#   mutate(comparison = factor(comparison,
#                             levels = c("bayesian_vs_basic", "adaptive_vs_basic", "adaptive_vs_bayesian"),
#                             labels = c("Bayesian vs Basic", "Adaptive vs Basic", "Adaptive vs Bayesian")))
# 
# p8 <- ggplot(differences_long, aes(x = difference)) +
#   geom_histogram(bins = 30, alpha = 0.7, fill = "lightgreen") +
#   geom_vline(xintercept = 0, linetype = "dashed", color = "red") +
#   facet_wrap(~comparison, scales = "free") +
#   labs(title = "Differences Between MPSA Methods",
#        x = "Difference", y = "Frequency") +
#   theme_minimal()

# === 9. ê²°ê³¼ ì €ì¥ ë° ë³´ê³ ì„œ ===
# 
# # ê²°ê³¼ ë””ë ‰í† ë¦¬ ìƒì„±
# if (!dir.exists("output/advanced_mpsa")) {
#   dir.create("output/advanced_mpsa", recursive = TRUE)
# }
# 
# # ëª¨ë“  ì‹œê°í™” ì €ì¥
# combined_plot1 <- (p1 + p2) / p3
# combined_plot2 <- (p4 + p5) / p7
# combined_plot3 <- p8
# 
# ggsave("output/advanced_mpsa/method_distributions.png", p1, width = 12, height = 10)
# ggsave("output/advanced_mpsa/posterior_distributions.png", p2, width = 12, height = 8)
# ggsave("output/advanced_mpsa/bayesian_uncertainty.png", p3, width = 12, height = 6)
# ggsave("output/advanced_mpsa/adaptive_weights.png", p4, width = 8, height = 6)
# ggsave("output/advanced_mpsa/adaptivity_scores.png", p5, width = 10, height = 6)
# ggsave("output/advanced_mpsa/pairwise_comparisons.png", p6, width = 12, height = 10)
# ggsave("output/advanced_mpsa/execution_times.png", p7, width = 10, height = 6)
# ggsave("output/advanced_mpsa/method_differences.png", p8, width = 12, height = 8)
# 
# # ì¢…í•© ì‹œê°í™” ì €ì¥
# ggsave("output/advanced_mpsa/comprehensive_analysis_1.png", combined_plot1, width = 16, height = 12)
# ggsave("output/advanced_mpsa/comprehensive_analysis_2.png", combined_plot2, width = 16, height = 12)
# 
# # ê³µê°„ ì§€ë„ ì €ì¥
# tmap_save(combined_maps, "output/advanced_mpsa/spatial_comparison.png")
# 
# # ìˆ˜ì¹˜ ê²°ê³¼ ì €ì¥
# write.csv(comparison_df, "output/advanced_mpsa/method_comparison_data.csv", row.names = FALSE)
# write.csv(uncertainty_df, "output/advanced_mpsa/bayesian_uncertainty_data.csv", row.names = FALSE)
# write.csv(stats_comparison, "output/advanced_mpsa/statistical_comparison.csv", row.names = FALSE)
# write.csv(differences_df, "output/advanced_mpsa/method_differences_data.csv", row.names = FALSE)
# write.csv(execution_times, "output/advanced_mpsa/execution_times.csv", row.names = FALSE)
# 
# # ê³µê°„ ë°ì´í„° ì €ì¥
# st_write(franklin_advanced, "output/advanced_mpsa/franklin_advanced_results.shp", delete_dsn = TRUE)

# === 10. ìµœì¢… ì¢…í•© í‰ê°€ ===
# 
# # ê° ë°©ë²•ë¡ ì˜ ì¥ë‹¨ì  ìš”ì•½
# method_evaluation <- data.frame(
#   Method = c("Basic MPSA", "Bayesian MPSA", "Adaptive MPSA"),
#   Computational_Speed = c("Fast", "Slow", "Medium"),
#   Uncertainty_Quantification = c("No", "Yes", "Limited"),
#   Spatial_Adaptivity = c("No", "No", "Yes"),
#   Interpretability = c("High", "Medium", "Medium"),
#   Best_Use_Case = c(
#     "Large datasets, quick analysis",
#     "Small datasets, uncertainty important",
#     "Complex spatial patterns"
#   )
# )
# 
# # ì„±ëŠ¥ ì§€í‘œ ìš”ì•½
# performance_summary <- list(
#   basic_mpsa = list(
#     mean = round(mean(mpsa_basic), 4),
#     sd = round(sd(mpsa_basic), 4),
#     range = round(range(mpsa_basic), 4)
#   ),
#   bayesian_mpsa = list(
#     mean = round(mean(bayesian_results$posterior_mean), 4),
#     sd = round(sd(bayesian_results$posterior_mean), 4),
#     range = round(range(bayesian_results$posterior_mean), 4),
#     avg_uncertainty = round(mean(bayesian_results$posterior_sd), 4)
#   ),
#   adaptive_mpsa = list(
#     mean = round(mean(adaptive_results$adapted_mpsa), 4),
#     sd = round(sd(adaptive_results$adapted_mpsa), 4),
#     range = round(range(adaptive_results$adapted_mpsa), 4),
#     avg_adaptivity = round(mean(adaptive_results$adaptivity_scores), 4)
#   )
# )
# 
# # ìƒê´€ê´€ê³„ ìš”ì•½
# correlations_summary <- data.frame(
#   Comparison = c("Basic vs Bayesian", "Basic vs Adaptive", "Bayesian vs Adaptive"),
#   Correlation = c(
#     round(cor(mpsa_basic, bayesian_results$posterior_mean), 3),
#     round(cor(mpsa_basic, adaptive_results$adapted_mpsa), 3),
#     round(cor(bayesian_results$posterior_mean, adaptive_results$adapted_mpsa), 3)
#   )
# )
# 
# # ì¢…í•© ë³´ê³ ì„œ ì €ì¥
# library(jsonlite)
# final_report <- list(
#   method_evaluation = method_evaluation,
#   performance_summary = performance_summary,
#   correlations_summary = correlations_summary,
#   execution_times = execution_times
# )
# 
# write_json(final_report, "output/advanced_mpsa/comprehensive_evaluation.json", pretty = TRUE)
# write.csv(method_evaluation, "output/advanced_mpsa/method_evaluation.csv", row.names = FALSE)
# write.csv(correlations_summary, "output/advanced_mpsa/correlations_summary.csv", row.names = FALSE)
# 
# # ìµœì¢… ìš”ì•½ ì¶œë ¥
# cat("=== ê³ ê¸‰ MPSA ë°©ë²•ë¡  ë¶„ì„ ì™„ë£Œ ===\n")
# cat("ğŸ“Š ì„±ëŠ¥ ë¹„êµ:\n")
# for (i in 1:nrow(stats_comparison)) {
#   cat(sprintf("  - %s: í‰ê·  %.4f (Â±%.4f)\n", 
#               stats_comparison$Method[i], 
#               stats_comparison$Mean[i], 
#               stats_comparison$SD[i]))
# }
# cat("ğŸ”— ìƒê´€ê´€ê³„:\n")
# for (i in 1:nrow(correlations_summary)) {
#   cat(sprintf("  - %s: %.3f\n", 
#               correlations_summary$Comparison[i], 
#               correlations_summary$Correlation[i]))
# }
# cat("â±ï¸  ê³„ì‚° íš¨ìœ¨ì„±:\n")
# for (i in 1:nrow(execution_times)) {
#   cat(sprintf("  - %s: %.2fì´ˆ\n", 
#               execution_times$Method[i], 
#               execution_times$Time_seconds[i]))
# }
# cat("ğŸ“ ê²°ê³¼ ì €ì¥ ìœ„ì¹˜: output/advanced_mpsa/\n")
# cat("ğŸ¯ ê³ ê¸‰ ë°©ë²•ë¡  ë¹„êµ ì™„ë£Œ!\n") 